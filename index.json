[{"uri":"https://phatpham14.github.io/fcj-fa25/4-eventparticipated/4.3-event4/","title":"Event 4: AWS Well-Architected Security Pillar Workshop","tags":[],"description":"","content":"Summary Report: “AWS Well-Architected Security Pillar Workshop” Event Information Time: 08:30 – 12:00, Saturday, November 29, 2025 (Morning Only). Location: AWS Vietnam Office, Bitexco Financial Tower, District 1, HCMC. Topic: Comprehensive Cloud Security based on the AWS Well-Architected Framework. Objectives Reinforce Security Mindset: Deeply understand the 5 pillars of security (Identity, Detection, Infrastructure, Data, Incident Response). Practical Application: Learn how to design systems adhering to \u0026ldquo;Zero Trust\u0026rdquo; and \u0026ldquo;Defense in Depth\u0026rdquo; principles. Future Preparation: Grasp the roadmap for advanced security certifications (AWS Security Specialty). Key Content \u0026amp; Agenda The workshop focused heavily on Security Best Practices, divided into in-depth sessions:\n1. Security Foundation (08:30 – 08:50) Core Principles: Least Privilege, Zero Trust, and Defense in Depth. Shared Responsibility Model: Clearly understanding the boundary of responsibility between AWS and the customer. Landscape: Top common security threats in the Vietnamese enterprise environment. 2. The Five Security Pillars (08:50 – 11:40) Pillar 1 - Identity \u0026amp; Access Management (IAM): Transitioning from IAM Users to IAM Identity Center (SSO). Using SCPs and Permission Boundaries for Multi-account management. Demo: Simulating access and validating IAM Policies. Pillar 2 - Detection: Continuous monitoring with GuardDuty and Security Hub. Automated Alerting using EventBridge. Pillar 3 - Infrastructure Protection: Network protection with WAF, Shield, and Network Firewall. VPC Segmentation and Security Groups vs. NACLs models. Pillar 4 - Data Protection: Encryption key management with KMS (Key rotation, policies). Protecting sensitive secrets with Secrets Manager. Pillar 5 - Incident Response (IR): IR Lifecycle according to AWS. Building Playbooks for scenarios: Compromised IAM keys, S3 public exposure, EC2 malware. Key Highlights \u0026amp; Takeaways Identity is the New Perimeter Previously, I focused mostly on Network Firewalls, but this workshop emphasized that in the Cloud, Identity is the most critical security layer. Strictly managing IAM Roles and Policies is the first and most effective line of defense. Automated Incident Response (IR Automation) The concept of \u0026ldquo;Automated Response\u0026rdquo; was truly impressive. Instead of waiting for human intervention during an incident, we can use Lambda functions to automatically isolate an infected EC2 instance or immediately revoke a compromised IAM key. Security Should Not Slow Down Development The \u0026ldquo;Security as Code\u0026rdquo; message helped me understand that security should be integrated into the DevOps process (DevSecOps) from the beginning, rather than being a bottleneck at the last minute. Event Photos Personal Reflection Even though it took place on a Saturday morning after completing the Final Showcase, this workshop still attracted my full participation due to its importance.\nSecurity knowledge is often dry, but the approach through practical Playbooks (such as handling S3 bucket exposure) made the lessons very accessible. This provides a solid foundation for me to be more confident when joining real-world enterprise projects after the internship, where security is always a top priority.\nConclusion: A perfect ending to the technical training series, completing the final piece of a Cloud Engineer: Security.\n"},{"uri":"https://phatpham14.github.io/fcj-fa25/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, I present an overview of my worklog.\nHow I completed it: I successfully completed this worklog through the First Cloud Journey internship program. My methodology involved a combination of self-study on AWS Skill Builder and Coursera, alongside the practical implementation of a real-world Group Project.\nDuration: The program spanned 12 weeks (approximately 3 months).\nWhat I did: During these weeks, I progressed from fundamental Cloud concepts and research skills to designing architectures, coding, implementing DevOps pipelines, and securing a complete application on AWS.\nBelow is the detailed content for each week:\nWeek 1: Getting familiar with FCJ culture and basic AWS services\nWeek 2: Deep diving into IAM, CLI, and Research Methods\nWeek 3: Understanding Load Balancing (ELB), Auto Scaling, and Monitoring\nWeek 4: Advanced Networking, Content Delivery, and Project Architecture\nWeek 5: Advanced Databases (NoSQL), Project Initialization, and Coursera Completion\nWeek 6: Containerization (Docker/ECS) and Backend Development\nWeek 7: DevOps Implementation: CI/CD Pipelines and Automation\nWeek 8: Frontend Development, System Integration, and Serverless\nWeek 9: Enhancing System Security and Code Refactoring\nWeek 10: Load Testing and Cost Optimization\nWeek 11: Project Documentation and Final Product Polishing\nWeek 12: Final Preparation, Rehearsal, and Readiness for Showcase\n"},{"uri":"https://phatpham14.github.io/fcj-fa25/4-eventparticipated/4.3-event3/","title":"Event 3: DevOps on AWS Workshop","tags":[],"description":"","content":"Summary Report: “DevOps on AWS Workshop” Event Information Time: 08:30 – 17:00, Monday, November 17, 2025. Location: AWS Vietnam Office, Bitexco Financial Tower, District 1, HCMC. Topic: DevOps Culture, Automation (CI/CD), Infrastructure as Code (IaC), and System Monitoring. Objectives Standardization: Understand DevOps culture and efficiency metrics (DORA metrics) to apply to teamwork processes. Automation: Master AWS Developer Tools (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) to build a complete CI/CD pipeline. Infrastructure Management: Adopt Infrastructure as Code (IaC) thinking with AWS CDK for more efficient resource management. Operations: Learn how to monitor and trace errors (Observability) for Microservices applications. Key Content \u0026amp; Agenda The full-day workshop covered intensive technical knowledge, divided into morning and afternoon sessions:\n1. Morning Session: DevOps Mindset \u0026amp; CI/CD (08:30 – 12:00) DevOps Culture: Understanding that DevOps is not just tools but a culture combining Development and Operations. Key metrics: MTTR (Mean Time To Recovery) and deployment frequency. AWS CI/CD Pipeline: Source Control: Managing source code with AWS CodeCommit (and GitFlow strategies). Build \u0026amp; Test: Configuring AWS CodeBuild to package applications. Deployment: Safe deployment strategies (Blue/Green, Canary) with AWS CodeDeploy. Orchestration: Automating the entire process using AWS CodePipeline. IaC (Infrastructure as Code): Comparison between CloudFormation (Templates) and AWS CDK (Code). Demo of infrastructure deployment using CDK. 2. Afternoon Session: Containers \u0026amp; Observability (13:00 – 17:00) Container Services: Managing Docker images with Amazon ECR. Container orchestration with Amazon ECS and EKS. Comparison of Microservices deployment on different platforms. Monitoring \u0026amp; Observability: Setting up Dashboards and Alarms with Amazon CloudWatch. Distributed tracing with AWS X-Ray to identify performance bottlenecks. Best Practices: Incident management strategies and AWS DevOps certification roadmap. Key Highlights \u0026amp; Takeaways IaC is a \u0026ldquo;Game Changer\u0026rdquo; Previously, my team often created resources by clicking on the Console (ClickOps), which is prone to errors and hard to synchronize. Through the AWS CDK demo, I realized the power of defining infrastructure using programming languages (TypeScript/Python). This makes reproducing environments (Dev/Test/Prod) easy and absolutely accurate. CI/CD is more than just Deploying Code I learned that a standard pipeline is not just pushing code to the server, but must include Automated Testing and Safe Deployment strategies (like Blue/Green) to minimize downtime risks for end-users. Observability \u0026gt; Monitoring Monitoring only tells you \u0026ldquo;The system is broken\u0026rdquo;, while Observability (with X-Ray) tells you \u0026ldquo;Why it broke\u0026rdquo;. With the current project\u0026rsquo;s Microservices architecture, integrating X-Ray is mandatory for effective debugging. Event Photos Personal Reflection The DevOps on AWS workshop was the final crucial technical piece I was missing for the Group Project.\nIt helped me systematize the entire software development process. Immediately after the session, I had ideas to refactor the team\u0026rsquo;s current GitLab CI pipeline, integrating ECR security scanning and X-Ray tracing to best prepare for the upcoming Final Showcase.\nConclusion: A day flooded with knowledge but incredibly valuable. I am now more confident to say that I understand standard DevOps processes!\n"},{"uri":"https://phatpham14.github.io/fcj-fa25/4-eventparticipated/4.2-event2/","title":"Event 2: AI/ML/GenAI on AWS Workshop","tags":[],"description":"","content":"Summary Report: “AI/ML/GenAI on AWS Workshop” Event Information Time: 08:30 – 12:00, Saturday, November 15, 2025. Location: AWS Vietnam Office, Bitexco Financial Tower, District 1, HCMC. Topic: Exploring the AI/ML landscape and practicing building Generative AI applications on AWS. Objectives Market Insight: Understand the context and development trends of AI/ML in Vietnam. Mastering Tools: Access the Amazon SageMaker platform for comprehensive Machine Learning processes (End-to-end ML). Approaching GenAI: Learn how to use Amazon Bedrock to build advanced Generative AI applications. Key Content \u0026amp; Agenda The workshop was divided into two in-depth technical sessions with practical Demos:\n1. Overview \u0026amp; Amazon SageMaker (09:00 – 10:30) AI/ML Landscape: Overview of the market and common use cases in Vietnam. Amazon SageMaker: Exploring the comprehensive ML lifecycle management platform: Data preparation \u0026amp; labeling. Model Training, Tuning, and Deployment. Integrated MLOps to automate processes. Live Demo: Walkthrough of the SageMaker Studio interface, visualizing how a model is built and deployed to production. 2. Generative AI with Amazon Bedrock (10:45 – 12:00) Foundation Models (FMs): Comparison and guidance on selecting foundation models like Claude, Llama, and Titan suitable for specific problems. Prompt Engineering: Techniques to optimize prompts: Chain-of-Thought reasoning, Few-shot learning. RAG \u0026amp; Agents: Retrieval-Augmented Generation (RAG) architecture combined with Knowledge Base to increase accuracy. Bedrock Agents: Building multi-step workflows and integrating external tools. Guardrails: Setting up content safety filters. Live Demo: Building a complete Generative AI Chatbot using Amazon Bedrock right in the session. Key Highlights \u0026amp; Takeaways SageMaker standardizes the ML process Previously, I thought ML workflows were very fragmented, but through the SageMaker demo, I saw the seamlessness from raw data processing to having an API endpoint. The built-in MLOps capabilities help minimize a lot of manual operational effort. The Power of Generative AI \u0026amp; RAG The most impressive part was the RAG technique. Combining proprietary data (Knowledge Base) with the power of LLMs (like Claude) solves the AI \u0026ldquo;hallucination\u0026rdquo; problem. This is key to applying AI to enterprise problems requiring high accuracy. Prompt Engineering is a new skill I learned that how you ask (Prompting) determines 50% of the quality of the AI\u0026rsquo;s answer. Techniques like \u0026ldquo;Chain-of-Thought\u0026rdquo; truly help the model think more logically. Event Photos Personal Reflection The AI/ML/GenAI on AWS workshop came at the perfect time as I was looking for solutions to integrate intelligent features into my Group Project.\nSeeing a GenAI Chatbot built in under 30 minutes thanks to Amazon Bedrock was truly impressive. It shifted my mindset from \u0026ldquo;building models from scratch\u0026rdquo; (very costly) to \u0026ldquo;leveraging existing models\u0026rdquo; (via API).\nConclusion: An extremely productive Saturday morning! I have added a powerful tool (Amazon Bedrock) to my toolkit.\n"},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.4-databaseandstorage/5.4.1-create-rds/","title":"Create RDS","tags":[],"description":"","content":" Open the Amazon Aurora and RDS\nIn left navbar, choose Databases, then click Create database In create console, choose Full configuration\nThen choose database type is MySQL\nChoose Templates is Production Select Availability and durability is Multi-AZ DB cluster deployment (3 instances)\nFill DB instance identifier\nChoose Self managed\nSpacific Master password In Instance configuration, choose db.m5d.large\nIn Storage, Allocated storage fill in 100 and Provisioned IOPS is 1000\nIn Connectivity, choose Don\u0026rsquo;t connect to an EC2, then select VPC Choose DB subnet group and then in VPC security group (firewall) select Choose existing\nSelect rds-sg\nThen click Create database "},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.3-vpc/5.3.1-create-route-table/","title":"Create Route table and Internet Gateway","tags":[],"description":"","content":" A Route Table is a set of rules (routes) used by a router to determine the best path for data packets to reach their destination.\nOpen the Amazon VPC console Choose Route tables, then click Create route table In the Create route table console: Specify name of Route table Choose VPC created Then click Create route table In Route table console, click Route table created Choose Route in navbar -\u0026gt; click Edit routes Edit routes console -\u0026gt; choose Target local -\u0026gt; Save changes Choose Internet Gateway in left navbar -\u0026gt; click Create internet gateway In Create Internet Gateway Specify name of Internet Gateway Click Create internet gateway Back to Route table -\u0026gt; Create route table like step 3 Click this Route table -\u0026gt; Edit routes In Target choose Internet Gateway created Then click **Save changes "},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"ApexEV — Workshop Introduction (Short) ApexEV is an enterprise-grade EV garage management platform that digitizes workshop operations, improves customer experience, and helps technicians work faster and safer.\nWhy this workshop:\nSolve common garage problems: manual processes, poor transparency, weak customer care, and data risk. Build a secure, scalable and cost-efficient cloud backend from day one using AWS best practices. Core architecture (summary):\nFrontend: React app hosted on AWS Amplify (CI/CD + CloudFront). Backend: Spring Boot services in ECS (Fargate) — serverless containers. Database: Amazon RDS (private subnets, automated backups, KMS encryption). Storage: Amazon S3 for media (use presigned URLs for direct uploads). API + Async: API Gateway as HTTPS ingress; SNS → Lambda → SES for email; Lambda → Bedrock for AI/chat. Network \u0026amp; Security: VPC (public/private), Security Groups, VPC Endpoints, WAF and least-privilege IAM. Key benefits:\nSecurity-first: backend and DB remain private; edge services terminate TLS and enforce WAF/rate limits. Cost-aware: Fargate + Lambda (pay-per-use), lifecycle rules and autoscaling reduce costs. Modern \u0026amp; modular: frontend/backend separation, event-driven async flows for resilience and scale. Workshop goals:\nProvision network and secure services, deploy frontend + backend, connect RDS and S3, and integrate email and AI pipelines. Each module includes steps, recommended settings and cleanup instructions. "},{"uri":"https://phatpham14.github.io/fcj-fa25/4-eventparticipated/4.1-event1/","title":"Event 1: AWS Cloud Day Vietnam 2025 - HCMC Connect Edition","tags":[],"description":"","content":"Summary Report: “AWS Cloud Day Vietnam 2025 – HCMC Connect Edition” Thông tin sự kiện Thời gian: Ngày 18/09/2025 (Song song với phiên chính tại Hà Nội). Địa điểm: Văn phòng AWS Vietnam, Tầng 26 \u0026amp; 36, Tòa nhà Bitexco Financial Tower, Quận 1, TP.HCM. Quy mô: Sự kiện vệ tinh (Satellite event) dành riêng cho cộng đồng \u0026ldquo;Builders\u0026rdquo; tại TP.HCM. Mục tiêu tham dự Cập nhật xu hướng: Tìm hiểu các công nghệ mới nhất về Generative AI và Modernization mà AWS đang đẩy mạnh tại thị trường Việt Nam. Thực hành (Hands-on): Tham gia các phiên workshop kỹ thuật để rèn luyện kỹ năng thực tế thay vì chỉ nghe lý thuyết. Networking: Kết nối với các chuyên gia AWS (Solution Architects), các nhà phát triển và cộng đồng công nghệ tại khu vực phía Nam. Nội dung chính \u0026amp; Agenda Sự kiện được chia thành hai phần chính: Livestream phiên toàn thể từ Hà Nội và các phiên chuyên đề (Breakout Tracks) tại chỗ.\n1. Plenary Session (Livestream) Theo dõi phiên khai mạc và các thông báo quan trọng về chiến lược Cloud \u0026amp; AI của AWS tại Việt Nam. Các diễn giả chính (Keynote Speakers) chia sẻ về tầm nhìn hạ tầng Cloud và tác động của AI đối với doanh nghiệp. 2. Breakout Tracks \u0026amp; Workshops Sự kiện tại TP.HCM tập trung vào 2 track kỹ thuật chính mà em rất quan tâm:\nTrack 1: Gen AI \u0026amp; Data\nTập trung vào cách xây dựng ứng dụng tích hợp Generative AI sử dụng Amazon Bedrock và Amazon Q Developer. Cách xây dựng nền tảng dữ liệu (Data Foundation) vững chắc để phục vụ cho AI. Track 2: Migration \u0026amp; Modernization\nCác chiến lược di chuyển lên mây (Migration) và hiện đại hóa ứng dụng (Modernization). Chuyển đổi từ kiến trúc Monolithic sang Microservices và Serverless (rất sát với dự án Group Project em đang làm). Diễn giả \u0026amp; Chuyên gia: Sự kiện có sự góp mặt của các chuyên gia uy tín như anh Nguyễn Gia Hưng, anh Eric Yeo, anh Toan Do, cùng đội ngũ Solution Architects của AWS Vietnam.\nĐiểm nhấn \u0026amp; Bài học (Key Takeaways) GenAI không còn là lý thuyết Qua các demo thực tế, em thấy việc tích hợp GenAI vào ứng dụng đã trở nên dễ dàng hơn nhờ các dịch vụ được quản lý (Managed Services) như Amazon Bedrock. Điều này mở ra hướng đi mới cho việc tối ưu hóa trải nghiệm người dùng trong các dự án tương lai. Hiện đại hóa là hành trình bắt buộc Tại Track 2, các diễn giả nhấn mạnh rằng \u0026ldquo;Modernization\u0026rdquo; không chỉ là chuyển đổi công nghệ (từ EC2 sang Lambda/Fargate) mà còn là thay đổi tư duy vận hành. Việc sử dụng Managed Services (như RDS, ElastiCache) giúp giảm tải gánh nặng quản trị để tập trung vào logic kinh doanh. Trải nghiệm Hands-on Workshop Khác với việc chỉ ngồi nghe, HCMC Connect Edition cho phép em trực tiếp thao tác trên môi trường AWS (Hands-on labs). Việc được các chuyên gia hướng dẫn fix lỗi trực tiếp tại chỗ giúp em hiểu sâu hơn về cách cấu hình bảo mật và tối ưu chi phí. Hình ảnh sự kiện Cảm nhận cá nhân Tham dự AWS Cloud Day - HCMC Connect Edition là một trải nghiệm rất khác biệt so với các sự kiện hội thảo thông thường.\nEm cảm nhận được sự gần gũi và tính thực tiễn cao. Thay vì chỉ nghe các bài diễn thuyết mang tính vĩ mô, em được ngồi cùng các \u0026ldquo;Builders\u0026rdquo; khác, cùng thảo luận về code, về kiến trúc và các vấn đề kỹ thuật cụ thể.\nSự kiện giúp em củng cố niềm tin vào con đường Cloud Engineer mà em đang theo đuổi. Đặc biệt, việc được tiếp cận sớm với các kiến thức về GenAI giúp em có thêm ý tưởng để đề xuất cải tiến cho dự án tốt nghiệp (Final Showcase) sắp tới của nhóm.\nLời kết: Một ngày \u0026ldquo;nạp\u0026rdquo; kiến thức đầy hiệu quả ngay tại \u0026ldquo;đại bản doanh\u0026rdquo; của AWS Vietnam!\n"},{"uri":"https://phatpham14.github.io/fcj-fa25/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Get acquainted with FCJ members - Read rules and regulations - Create AWS free tier account - Get familiar with AWS Console interface - Finished module 1 09/09/2025 09/09/2025 https://policies.fcjuni.com/ 2 - Read guide and installed neccessary extensions to create a site for worklog and others in the future - Started Module 2: + Learned about AWS Virtual Private Cloud + Understand the concept of VPC + Get to know about VPC Security and others VPC feature 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Watched Module 2 lab 3 to get an overview of the topic - Practiced before implementation 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Launched EC2 Instance - Testing Amazon Bedrock Playground feature - Created a web using Lambda service - Created a database with RDS service for testing purpose 09/12/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nLearned how to reduce costs by choosing AWS services that align with specific project requirements\n"},{"uri":"https://phatpham14.github.io/fcj-fa25/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Pham Do Dinh Phat\nPhone Number: 0963646027\nEmail: PhatPDDSE192630@FPT.EDU.VN\nUniversity: FPT University\nMajor: Software Engineering\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/03/2026\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://phatpham14.github.io/fcj-fa25/3-blogstranslated/3.3-blog3/","title":"Proactive Strategies for Cyber Resilience and Business Continuity on AWS","tags":["AWS Public Sector","Best Practices","Disaster Response","Security"],"description":"","content":"By Devin Gordon and Henrik Balle\nAmazon Web Services (AWS) recommends that organizations prepare to recover workloads in the event of cybersecurity incidents or business continuity events such as natural disasters or technical failures. In this post, we provide guidance and strategies for Public Sector organizations to use AWS infrastructure to operate resilient systems in the cloud.\nAWS recommends that its customers:\nUse cybersecurity frameworks and AWS architectural best practices. Implement a multi-account environment. Use infrastructure as code (IaC) to deploy AWS environments and workloads. Prepare a recovery account in an Availability Zone or Region different from the primary workload. Include all application code, IaC code, configuration files, and other dependencies in the recovery account. Define a data backup strategy to the recovery account. Implement automated testing (unit and full workload) in the recovery account. Use an established framework As cybersecurity incidents like ransomware increase, Public Sector organizations look to frameworks such as the NIST Cybersecurity Framework (CSF) 2.0 from the National Institute of Standards and Technology (NIST) to guide better cybersecurity risk governance. While the CSF organizes cybersecurity outcomes by functions (govern, identify, protect, detect, respond, recover), NIST does not prescribe specific ways to achieve those outcomes.\nFor more specific guidance, refer to resources such as:\nAWS Blueprint for Ransomware Defense: Provides a mapping of AWS services to CSF functions. AWS Well-Architected Framework: Dives into six pillars to help cloud architects build secure, high-performing, resilient, and efficient infrastructure, following the AWS Shared Responsibility Model. These frameworks are complemented by the AWS Security Reference Architecture (AWS SRA), which helps you design, deploy, and manage AWS security services.\nImplement a multi-account strategy on AWS AWS recommends following best practices when deploying cloud environments by using a multi-account strategy, also known as a landing zone. Using AWS Organizations along with AWS Control Tower or Landing Zone Accelerator on AWS creates an environment suitable for management and automation. This helps isolate and manage applications and data, and eliminate unintended lateral movement.\nIn a multi-account strategy, AWS recommends centrally managing identities using AWS IAM Identity Center.\nNote: In a recovery scenario, you may need to use root credentials. Root credentials should be properly secured to prevent account takeover. AWS recently announced new capabilities for centrally managing root credentials to strengthen security posture.\nBuild your AWS infrastructure with automation As a key principle of DevSecOps, AWS recommends deploying all AWS infrastructure using Infrastructure as Code (IaC). IaC helps iterate infrastructure quickly, improve consistency, reduce configuration errors, and crucially serves as documentation for infrastructure to accelerate recovery.\nSupporting tools and services:\nAWS CloudFormation: Define infrastructure using templates. You can also create a stack from existing infrastructure. AWS Cloud Development Kit (AWS CDK): Define resources using programming languages. AWS Serverless Application Model (AWS SAM): An open-source framework for serverless applications. HashiCorp Terraform: A multi-platform IaC tool. IaC code should be stored in a source code repository (e.g., GitLab). You can use the DevOps Pipeline Accelerator (DPA) to build a standardized CI/CD pipeline.\nPrepare a recovery location AWS recommends establishing separate accounts dedicated to recovery to deal with cybersecurity incidents.\nTo deal with technical failures or natural disasters (e.g., AWS Availability Zone outage):\nIf using the same Region, ensure the AZ used for recovery is discrete from the primary workload. Use Availability Zone IDs (AZ IDs) to ensure consistency across accounts (since AWS randomizes AZ names between accounts). Figure 1. Example of workload and recovery account using Availability Zone ID.\nFor higher fault tolerance, consider multi-Region recovery. For example: Primary Region in US East (N. Virginia) and backup in US East (Ohio). Evaluate your encryption key strategy with AWS Key Management Service (AWS KMS) to decrypt cross-region backup data.\nDefine a backup strategy and implement automated testing AWS recommends aligning your backup strategy with cloud-based solutions:\nUse AWS Backup to back up workload data, IaC, infrastructure configuration, application code, and AMIs to the recovery account. Consider an Immutable backup strategy with AWS Backup Vault Lock to protect against ransomware. Automated Testing: After setting up the recovery account, implement automated testing to validate the viability of backup data. You should periodically build the full workload in the recovery account using IaC code and backups to ensure the Business Continuity process runs smoothly.\nConclusion By following the best practices in this post, Public Sector organizations can use AWS capabilities to prepare for recovery from a business continuity event. Please contact your AWS account team and solutions architect for further guidance.\nAbout the Authors Devin Gordon\nPrincipal Solutions Architect at Amazon Web Services (AWS), supporting federal civilian agencies. He specializes in security on AWS and is passionate about helping customers deploy secure environments while leveraging emerging technologies. He enjoys SCUBA diving and rock climbing.\nHenrik Balle\nPrincipal Solutions Architect at AWS, supporting the US public sector. He works closely with customers on a wide range of topics from machine learning to security and governance at scale. He enjoys road biking and home renovation projects.\n"},{"uri":"https://phatpham14.github.io/fcj-fa25/3-blogstranslated/3.2-blog2/","title":"AWS IoT Greengrass Nucleus Lite - Revolutionizing Edge Computing on Resource-Constrained Devices","tags":["AWS IoT","AWS IoT Greengrass V2","Edge Computing","IoT Edge"],"description":"","content":"By Camilla Panni and Greg Breen\nAWS IoT Greengrass is an open-source edge runtime and cloud service that helps you build, deploy, and manage multi-process applications at scale across fleets of IoT devices.\nAWS IoT Greengrass V2 was launched in December 2020 with a Java-based edge runtime called nucleus. With release 2.14.0 in December 2024, AWS introduced an additional edge runtime option: Nucleus Lite - written in C.\nAWS IoT Greengrass Nucleus Lite is a lightweight, open-source edge runtime designed for resource-constrained devices. It extends AWS IoT Greengrass capabilities to lower-cost devices and single-board computers serving applications such as smart home hubs, smart energy meters, smart vehicles, edge AI, and robotics.\nThis article explains the advantages of both edge runtime options and provides guidance to help you choose the right approach for your use case.\nKey differences between Nucleus and Nucleus Lite AWS IoT Greengrass Nucleus Lite is fully compatible with the AWS IoT Greengrass V2 Cloud Service API and IPC interface. This means you can build and deploy components targeting one or both runtimes. However, Nucleus Lite has distinct technical characteristics:\n1. Memory footprint Nucleus (Java): Requires a minimum of 256 MB disk and 96 MB RAM. AWS recommends a minimum of 512 MB RAM to support the OS, JVM, and your applications. Nucleus Lite (C): Requires less than 5 MB RAM and 5 MB storage, does not depend on a JVM, and only requires the C Standard Library. Figure 1: Comparison of Nucleus vs. Nucleus Lite memory footprint.\nThis smaller footprint opens up new possibilities for IoT application development on resource-constrained devices.\n2. Static memory allocation Nucleus Lite allocates a fixed amount of memory at startup and keeps it unchanged.\nBenefit: Deterministic resource requirements, minimizing the risk of memory leaks. Performance: Eliminates non-deterministic latency often found in garbage-collected languages like Java. 3. Directory structure Nucleus Lite clearly separates components on disk, offering optimized support for Embedded Linux:\nRuntime: Can be stored on a Read-Only partition (supporting A/B OS update mechanisms). Components \u0026amp; Config: Located on a Read-Write partition or overlay for management via deployments. Logs: Stored in a temporary partition or separate volume to prevent flash memory wear. This separation helps you create golden images for mass device manufacturing.\n4. Systemd Integration Systemd is a mandatory requirement for Nucleus Lite.\nNucleus Lite is installed as a set of systemd services. Each deployed component also runs as a separate systemd service. Logging is handled centrally by systemd, allowing the use of standard Linux tools for debugging. Choosing between Nucleus and Nucleus Lite The choice depends on your specific use case, hardware constraints, and operating system. The table below summarizes the recommendations:\nWhen to use Nucleus (Java) When to use Nucleus Lite (C) You want to use Windows or a Linux distro without systemd Your device is memory constrained (≤ 512 MB RAM) Your application runs in Docker containers Device CPU clock speed is \u0026lt; 1 GHz Application components are Lambda functions You use Embedded Linux and need strict partition control (OS updates, A/B partitions) You develop using interpreted or scripting languages You program using compiled languages You need features not yet supported by Nucleus Lite You have compliance requirements that disallow Java usage You are creating an AWS IoT SiteWise gateway You prioritize static memory allocation Table 1: Guidance for choosing between Nucleus and Nucleus Lite.\nUse Cases and Deployment Scenarios 1. Use Cases With low resource requirements, Nucleus Lite is suitable for low-cost devices with limited memory and CPU in sectors such as smart homes, industrial, automotive, and smart metering.\n2. Embedded Systems Nucleus Lite supports Embedded Linux out-of-the-box via the meta-aws project. This project provides \u0026ldquo;recipes\u0026rdquo; for integration into Yocto/OpenEmbedded, and the meta-aws-demos repository contains examples such as A/B updates using RAUC.\n3. Multi-tenancy with containerized Nucleus Lite With its small footprint, Nucleus Lite enables efficient containerization for multi-tenant models.\nFigure 2: Multi-tenant containerization.\nSecure Isolation: Each container instance maintains strict boundaries. Resource Optimization: Small footprint allows multiple containers to operate on limited devices. Independent Operation: Applications are managed and updated separately. Best Practices 1. Plugin compatibility: Nucleus plugins (written in Java for the original runtime) cannot be used with the Nucleus Lite runtime.\n2. Component Language Selection:\nChoosing Python/Java will reduce the memory-saving advantages of Nucleus Lite (due to the need for an interpreter or JVM). Recommendation: Rewrite critical components in C, C++, or Rust to achieve maximum performance. 3. Migration and Development:\nWhen migrating: You can run existing components \u0026ldquo;as-is\u0026rdquo; to ensure functionality before optimizing. When calculating memory budgets: Account for all runtime dependencies and system footprint, not just Nucleus Lite itself. Future and Conclusion AWS IoT Greengrass Nucleus Lite helps reimagine edge computing deployments by significantly reducing resource requirements. This allows you to:\nDeploy IoT on cheaper, more diverse hardware. Reduce operational costs. Enable new use cases that were previously unfeasible. Ready to get started?\nExplore: AWS IoT Greengrass Documentation. Get hands-on: Installation Guide. Community: Join the AWS IoT Community Forum. Contribute: Submit code to the Github repository. About the Authors Camilla Panni\nSolutions Architect at AWS. She supports Public Sector customers in Italy to accelerate cloud adoption. Camilla has a technical background in automation and IoT.\nGreg Breen\nSenior IoT Specialist Solutions Architect at AWS, based in Australia. With deep experience in embedded systems, Greg supports product development teams across the Asia-Pacific region in bringing devices to market.\n"},{"uri":"https://phatpham14.github.io/fcj-fa25/3-blogstranslated/3.1-blog1/","title":"Characteristics of Financial Services HPC Workloads on the Cloud","tags":["FSI","HPC","Best Practices"],"description":"","content":"By Mark Norton and Flamur Gogolli\nThis post introduces the High Performance Computing (HPC) requirements in the Financial Services Industry (FSI). It defines the technical attributes of compute-heavy workloads and discusses their essential characteristics. Additionally, it provides a decision tree process to help customers select the right HPC platform on the cloud—whether from commercial vendors, open-source, or cloud-native solutions—based on customer and workload requirements.\nWhile it cannot cover every possible option, this article provides guidance on how to achieve price-performance goals and solve HPC challenges using AWS services and solutions, based on experience with Financial Services customers.\nIn the financial services sector, HPC (also known as Grid computing) has long been used to run simulations and solve complex challenges.\nCapital Markets: HPC systems are used to price financial instruments (e.g., stocks, ETFs, derivatives, bonds) using mathematical models like Monte Carlo. Utilization models range from real-time/intraday latency-sensitive calculations to large batch workloads that typically run overnight. Investment Management: HPC is used for pricing and risk analysis, exposure calculation, and the definition, optimization, and back-testing of hedging strategies. Insurance: HPC use cases include actuarial modeling and catastrophe simulations to help insurers understand losses, set premium levels, and develop risk mitigation strategies. Cloud adoption patterns in FSI Financial institutions face increasing demand for HPC resources due to regulatory requirements, market volatility, and reporting needs—requiring massive capacity during periods like overnight processing or end-of-quarter reporting.\nMany organizations are moving from on-premises infrastructure to the cloud to scale resources elastically and optimize costs. FSI customers typically migrate their HPC platforms to the cloud in phases:\nAll On-premises: Scheduler, compute, and data sources are hosted on-site. Hybrid Burst: Scheduler, compute, and data remain on-premises but are supplemented by cloud compute capacity during periods of high or burst demand. Lift and Shift: The existing scheduler is moved \u0026ldquo;as-is\u0026rdquo; to the cloud, running compute nodes on the cloud with configurations similar to on-premises. AWS Optimized: The focus is on compute elasticity and using managed services combined with purchasing models (Amazon EC2 Spot, Savings Plans). AWS Native: A complete reimagining of traditional HPC platforms, emphasizing cloud-native architectures, serverless building blocks, and optimized hardware (AI accelerators). Workload characteristics for architecture decisions HPC workloads in FSI have diverse requirements, but some key common characteristics include:\nVarying Task Duration: From a few seconds to hours or even days. High Throughput Requirements: Often need to process tens of thousands of tasks per second. High Parallelization Requirements: Most calculations are typically loosely coupled, allowing parallel execution. Resource Intensity: CPU : Memory ratios and I/O requirements. Software Requirements: Specific operating systems, platforms, and libraries. Data \u0026amp; Application Dependencies: Local vs. distributed processing. Elasticity \u0026amp; Flexibility: The ability to scale up and down based on demand. Cost Optimization: Balancing performance and cost. We will use the decision tree in Figure 1 to guide the selection of the appropriate platform.\nFigure 1. Decision tree for selecting an HPC solution on the cloud based on workload characteristics.\nNavigation begins with a critical decision: Keep the existing on-premises scheduler or Build a new cloud-native solution.\n1. Migrating your existing scheduler to the cloud If you choose to keep your existing Grid scheduler due to migration effort concerns or project timelines:\nCommercial Schedulers: IBM Spectrum Symphony or TIBCO DataSynapse GridServer®. Open-source Schedulers: Slurm Workload Manager or HTCondor. Common migration methods:\nHybrid Burst: Use AWS to supplement the on-premises grid during peaks. Lift-and-Shift: Move the entire system to AWS \u0026ldquo;as-is\u0026rdquo;. 2. Building and using a cloud-native HPC scheduler / platform This approach addresses legacy limitations and maximizes cloud benefits. The decision relies heavily on Task Duration:\nMixed Short/Long Tasks (Seconds - Minutes): If familiar with Slurm: Use AWS ParallelCluster or AWS Parallel Computing Service (managed service). Long Tasks (\u0026gt; 1 minute): Use AWS Batch, a fully managed batch computing service (free scheduler, pay only for compute resources). High Throughput (Seconds - Minutes): Use HTC-Grid, a community project (FINOS) capable of processing tens of thousands of tasks per second. Very Short Tasks (\u0026lt; 15 minutes): Leverage serverless architecture with AWS Lambda. No \u0026ldquo;always-on\u0026rdquo; server management, high throughput, but subject to strict execution time limits. Conclusion There is no \u0026ldquo;one-size-fits-all\u0026rdquo; solution. The number of tasks and their duration will determine the most suitable solution.\nView platform selection as the start of a journey. We often see customers continuing their journey from \u0026ldquo;Lift \u0026amp; Shift\u0026rdquo; to \u0026ldquo;AWS Optimized\u0026rdquo; and \u0026ldquo;AWS Native\u0026rdquo; for continuous optimization.\nKey Takeaways If maintaining an existing scheduler: Commercial solutions (IBM, Tibco) offer stability but may lack cloud flexibility. Open-source solutions (Slurm, HTCondor) can utilize AWS ParallelCluster/PCS for better management. "},{"uri":"https://phatpham14.github.io/fcj-fa25/3-blogstranslated/3.4-blog4/","title":"The Frugal HPC Architect – Ensuring Effective FinOps for HPC Workloads at Scale","tags":["HPC","FinOps","Best Practices"],"description":"","content":"By Alex Kimber\nAdopting flexible, on-demand, and scalable computing in the cloud brings many benefits. It frees organizations from long, repetitive procurement processes associated with on-premises infrastructure, which often require educated guesses for sizing, resulting in infrastructure that is only as good as the day it was installed. While the focus of this article is on cost reduction, it is also important to recognize the opportunities that testing at scale can unlock. The cloud allows customers to achieve levels of scale that would be financially impossible with on-premises infrastructure because it can be provisioned for the duration of any individual workload and then released. However, with flexible provisioning in the cloud comes cost variability that can concern organizations with fixed budgets or tight profit margins.\nIn this article, we will explore best practices we\u0026rsquo;ve seen from our work with customers across various industries running HPC workloads at scale. Additionally, we will reference guidance from Werner Vogels, an industry leader who developed \u0026lsquo;The Frugal Architect\u0026rsquo; to help organizations think about these challenges and find the right balance between the competing requirements of flexibility, efficiency, and system performance.\nHPC and the Cloud High Performance Computing (HPC) systems are becoming increasingly common on AWS and were among the first workloads to leverage flexible computing at scale, whether supplementing on-premises capacity with \u0026lsquo;burst\u0026rsquo; models or as a primary venue for large-scale computing. Although lessons from The Frugal Architect are broadly applicable, there are some specific lessons that apply to HPC workloads.\nHPC includes both tightly coupled systems running workloads such as Computational Fluid Dynamics (CFD), or massive loosely coupled systems that might run financial models. Both have the potential to generate significant costs for their consumers, so careful consideration of the design, measurement, and observability of these systems is all the more important. This article will focus on opportunities for loosely coupled workloads, but many of these will apply to any HPC workload.\nCost and Frugal Tools The first \u0026lsquo;Law\u0026rsquo; of The Frugal Architect is “Make Cost a Non-Functional Requirement”, emphasizing the importance of considering cost alongside other requirements such as security, accessibility, compliance, and maintainability as measures of a system\u0026rsquo;s success.\nIn a \u0026lsquo;pay as you go\u0026rsquo; provisioning model, cost is the product of unit cost and the number of units consumed. Optimization can come from reducing the cost of a given unit, consuming fewer units – or both. You can achieve this in various ways for HPC systems running on the cloud, and AWS has services and features to support each method.\nReducing Unit Cost AWS offers many opportunities to reduce unit costs when using its services. Each strategy has trade-offs that may or may not affect application performance, but once evaluated and deployed, can yield significant benefits.\nOne high-impact example of reducing compute unit costs is Amazon EC2 Spot, which allows customers to save up to 90% compared to on-demand provisioning for the same compute capacity. The trade-off with EC2 Spot is that these instances come from pools ready to serve on-demand consumers, and thus they can be reclaimed by AWS at any time with a 2-minute warning. HPC workloads that can tolerate intermittent interruptions (either because individual tasks are short or because they can checkpoint their progress) can realize significant savings with EC2 Spot with minimal disruption.\nThere are also commercial offerings for customers with a steady level of demand. In this scenario, Amazon EC2 Savings Plan allows customers to save up to 72% in exchange for committing to that capacity for 1 or 3 years. Typically, customers will combine EC2 Spot, Savings Plans, and on-demand capacity to achieve an optimal mix.\nBeyond commercial solutions, there are technical opportunities to reduce unit compute costs by ensuring you only provision what you need. AWS currently offers over 750 EC2 instance types, so customers can find the right instance type for their workload. This represents a shift from traditional on-premises provisioning, where homogeneous deployments are used to support the broadest possible set of tasks. On the cloud, each workload can provision the right mix of CPU, GPU, memory, storage, and network performance, avoiding paying for unused capabilities and features.\nBeyond compute instances, AWS offers a range of data storage services that present opportunities to reduce unit storage costs. For example, depending on the characteristics of a given workload, it may be optimal to use Amazon Simple Storage Service (Amazon S3) – possibly with Mountpoint if you need POSIX access – instead of a traditional network file system. From there, you can explore services like Amazon S3 Intelligent-Tiering to further reduce unit costs per GB of storage if there are infrequently accessed files.\nFor every application, there may be several opportunities to reduce the unit cost of resources on AWS. By analyzing potential benefits and trade-offs, you can prioritize options and explore those with the most potential benefit for any individual workload, avoiding a \u0026lsquo;one size fits all\u0026rsquo; approach.\nReducing Unit Consumption Consumed units will vary depending on the AWS service you are using. Some services even have multiple unit types, covering different usage patterns (e.g., Amazon S3 has consumption units for GET operations as well as for GB of data stored).\nThere are several effective ways to reduce Amazon EC2 core-hour unit consumption; for example, you can increase efficiency by ensuring that – as much as possible – cores are performing compute tasks and not idling or working on non-compute activities. Or you can monitor effectiveness by ensuring the value of the work in progress exceeds the cost of the provisioned compute resources; otherwise, consider not running that workload. This aligns with the second law of The Frugal Architect – “Systems that Last Align Cost to Business”, ensuring that any cost increase is understood in the context of revenue.\nThe focus on efficiency and effectiveness represents a significant difference from on-premises task scheduling, where capacity is static. In that scenario, low-value or inefficient workloads might be acceptable because they incur no additional cost and can be run at low priority. While this is also true on the cloud when capacity is covered by a Savings Plan, any additional capacity provisioned incurs additional cost that needs to be evaluated against revenue value.\nOne group of methods to reduce compute unit consumption is to analyze the billing lifecycle of an AWS instance, from the moment the operating system (OS) starts booting until termination. This time can include overhead activities such as: OS boot, downloading and installing binaries, launching HPC client agents, connecting to the scheduler, idle time with no tasks, or tasks waiting for dependent data. By minimizing time spent on overhead and maximizing time spent on useful computation, you can improve system efficiency, reducing the number of compute units needed for a given workload.\nTotal OS boot time costs can be reduced by: optimizing individual boot processes, reducing the number of instance boots by using long-running On-Demand Instances, or diversifying instance selection to minimize EC2 Spot Interruption events requiring replacement instance launches. CPU cores waiting for data can be kept busy by a level of task oversubscription, for example running 10 threads on a system with 8 vCPUs.\nArm-based AWS Graviton instances can deliver significant price/performance benefits compared to x86 instances running HPC workloads. Graviton instances provide up to 40% better price performance while using up to 60% less energy than comparable EC2 instances. Since AWS Graviton processors execute the Arm64 instruction set, you may need to take some steps to port applications, but AWS provides the AWS Porting Advisor for Graviton to assist with this. You can track the impact of Graviton adoption using the AWS Graviton Savings Dashboard. Performance benefits from Graviton can help you complete workloads faster for the same cost, or reduce cost by provisioning fewer instances for the same duration.\nA good metric to evaluate success in reducing unit consumption is CPU utilization. If your Amazon EC2 instances have periods of significantly low utilization, you might simply need to use fewer instances, but run them at higher utilization.\nHPC applications using large data can also benefit from opportunities to reduce high-performance storage consumption. For example, many HPC workloads on the cloud use the Amazon FSx for Lustre filesystem, providing throughput of hundreds of GB/s and millions of IOPS. But storing the entire dataset on Lustre might not be justifiable. Typically, customers link the filesystem with Amazon S3. In this way, FSx for Lustre presents Amazon S3 objects as files, which can be imported into Lustre on demand. This significantly reduces the size of the Lustre filesystem while maintaining high performance. Further reductions can be achieved by enabling Lustre data compression, reducing the filesystem capacity needed for a given dataset.\nAnother method is to use Amazon File Cache, providing high-performance access to Amazon S3 object storage while caching files stored in on-premises NFSv3 filesystems, reducing the cost of duplicate data storage on the cloud. Additional savings can be achieved for both Amazon File Cache and FSx for Lustre by turning them off when not in use and recreating them when demand returns.\nObservability The fourth law of The Frugal Architect is “Unobserved Systems lead to unknown costs”. In the previous section, we introduced core concepts of efficiency (ensuring provisioned resources spend the least time on overhead work) and effectiveness (ensuring work value exceeds the cost of provisioned compute resources).\nBoth are hard to evaluate without observability systems, which help HPC managers know overall efficiency through metrics like computation as a percentage of capacity provisioned, and help customers know effectiveness through measures of business value as a percentage of cost to compute (hopefully over 100%).\nBy providing these metrics in a timely and granular manner, stakeholders can identify and quantify continuous improvement opportunities, measure the results of changes, and detect unexpected outcomes quickly. AWS provides many tools supporting observability across metrics, logs, and traces, such as Amazon CloudWatch or Amazon Managed Service for Prometheus.\nIt is also important to have clear visibility into cost (and if possible, cost attribution), and have tools to support accessing, analyzing, and understanding cost data. For example, AWS Data Exports allows exporting data in the FinOps Open Cost and Usage Specification (FOCUS) 1.0 standard, which can be processed by existing reporting solutions or visualized with Amazon QuickSight. AWS Cost Explorer also allows granular cost exploration, from high-level overviews to deep dives to identify trends, anomalies, or key cost drivers.\nFinally, AWS Compute Optimizer helps identify underutilized resources and recommends how to address them. For example, it might suggest choosing an instance with less memory if the current provision is underused. Recommendations come with potential savings to help you make better decisions.\nConclusion When planning migration or expanding HPC workloads to the cloud, cost needs to be considered alongside other non-functional requirements. This article explored the shift in cost drivers when moving systems to the cloud, and how mechanisms applicable to on-premises clusters may no longer be suitable.\nHPC systems can benefit greatly from the flexibility and scalability of the AWS Cloud; costs can therefore be significant and need to be clearly understood. Cost generation varies depending on how compute, storage, and other resources are used on the cloud, requiring different cost management than fixed-capacity on-premises infrastructure.\nAWS provides many effective mechanisms to maximize the scale and elasticity the cloud offers, while ensuring you can understand and manage costs. With guiding principles like those in The Frugal Architect, AWS tools and frameworks help build efficient, flexible, and robust architectures to support HPC workloads.\nConsider the AWS Well-Architected Framework, which provides a dedicated pillar on cost-optimization alongside other non-functional keys like security, reliability, and sustainability.\nAbout the Author Alex Kimber\nAlex Kimber is a Principal Solutions Architect at AWS Global Financial Services, with over 20 years of experience building and operating high-performance grid computing platforms at investment banks.\n"},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.4-databaseandstorage/5.4.2-create-s3/","title":"Create an AWS S3","tags":[],"description":"","content":"In this section you will create S3 bucket to storage photos\nOpen the Amazon S3 Click Create bucket In create console, fill in name of bucket Then leave everything as default like picture Click Create bucket bottom "},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.3-vpc/5.3.3-create-security-groups/","title":"Create Security Groups","tags":[],"description":"","content":" Open the Amazon VPC console Choose Security Groups -\u0026gt; click Create security group In Create Security group Spacific name of Security group Choose VPC created Add rule Inbound and Outbound for Security Group In ReGenZet project we have 4 Security groups, which are fargate-sg, rds-sg, alb-sg and endpoint-sg. fargate-sg is security group for AWS ECS Fargate Do again step 1 -\u0026gt; 3 Choose fargate-sg created Inbount Add rule security group of Application Load Balancer (ALB) is alb-sg Outbound Add rule security group of MySQl (rds-sg) and HTTPS Click Create security group rds-sg is security group for AWS RDS Do again step 1 -\u0026gt; 3 Choose rds-sg created Inbound Add rule security group of MySQL like this instruct Outbound is not Add rule Click Create security group alb-sg is security group for AWS Application Load Balancer (ALB) Do again step 1 -\u0026gt; 3 Inbound Add rule HTTPS and HTTP type like this instruct Outbound Add rule security group of AWS ECS Fargate (fargate-sg) Click Create security group endpoint-sg is security group for VPC Endpoints Do again step 1 -\u0026gt; 3 Inbound Add rule security group of AWS ECS Fargate (fargate-sg) Outbound is not Add rule "},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.3-vpc/5.3.2-create-subnets/","title":"Create Subnets","tags":[],"description":"","content":"Create Public Subnet Open the Amazon VPC console Choose Subnets -\u0026gt; click Create subnet In Create subnet console: Choose VPC created Fill subnet name Choose AZ Spacific IPv4 subnet Then click Create subnet Create Private Subnet Do again step 1 -\u0026gt; 4 Click this Subnet -\u0026gt; Choose Route table Choose Route table ID is private Click Save "},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.3-vpc/5.3.4-create-vpc-endpoints/","title":"Create VPC Endpoints","tags":[],"description":"","content":" Open the Amazon VPC console Choose Endpoints -\u0026gt; click Create endpoints In Create console: Fill name of VPC endpoint Type is AWS Services Search In this project we have 5 VPC Endpoints VPC Enpoint S3 Gateway (apexev-s3-gateway) In search box -\u0026gt; com.amazonaws.ap-southeast-1.s3 Choose type Gateway Choose VPC created Choose Route table private Policy is Full access Click Create endpoint VPC Enpoint ECR API \u0026amp; DKR Interface In search box -\u0026gt; ecr Will see ecr.api and ecr.dkr interface Do this twice and choose a different one each time Choose VPC created Tick two subnet private in two difference AZ Choose endpoint-sg Click Create endpoint VPC Enpoint Logs In search box -\u0026gt; com.amazonaws.ap-southeast-1.logs Choose com.amazonaws.ap-southeast-1.logs Choose VPC created Tick two subnet private in two difference AZ Choose endpoint-sg Click Create endpoint VPC Enpoint AWS SNS In search box -\u0026gt; com.amazonaws.ap-southeast-1.sns Choose com.amazonaws.ap-southeast-1.sns Choose VPC created Tick two subnet private in two difference AZ Choose endpoint-sg Click Create endpoint "},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.2-project-architecture/","title":"Project Architecture","tags":[],"description":"","content":"Architecture Overview This document describes the recommended production-ready architecture for ReGenZet — an EV Garage Management System — and explains the technology choices used in the workshop.\nFrontend: A React single-page application deployed and hosted via AWS Amplify. Amplify provides automated CI/CD, asset hosting and integration with CloudFront for global caching and fast client delivery. Backend: Containerized Spring Boot services packaged as Docker images and deployed to Amazon ECS (Fargate). Fargate removes host management overhead and provides scalable, serverless compute for microservices. Database: Amazon RDS (MySQL/Postgres) hosted in private subnets. RDS provides automated backups, snapshots, Multi-AZ options and encryption at rest (KMS). API Management: Amazon API Gateway exposes a single HTTPS entry point for client traffic, handles request routing, TLS termination and request throttling. Media Storage: Amazon S3 holds vehicle inspection images, videos and other media. Use presigned URLs for secure direct-upload/download to offload traffic from the backend. Asynchronous / Serverless components: Email pipeline: Backend (Spring) publishes an event to SNS, which triggers a Lambda to send email via Amazon SES. AI/Chat pipeline: Frontend → API Gateway → Lambda → Amazon Bedrock (or other managed LLM) for inference and conversational workflows. Network \u0026amp; Security: Deploy resources inside a VPC with well-defined Public and Private subnets. Use Security Groups and NACLs to control traffic. Use VPC Endpoints (Gateway and Interface) for S3 and private service access, keeping traffic inside the AWS network. Why this architecture? Security-first\nThe backend and RDS instances reside in private subnets and never expose database ports to the public internet. API Gateway and load-balanced frontends terminate TLS at the edge, while internal services communicate over private networking. Cost-efficiency\nFargate and Lambda offer a pay-for-what-you-use model. When appropriate, consider Fargate Spot for non-critical workloads and configure autoscaling and lifecycle policies for S3 to reduce costs. Operational simplicity \u0026amp; modern patterns\nClear separation of concerns between frontend and backend, with API Gateway as a single ingress point. Event-driven components (SNS, Lambda) decouple email and AI processing from request-response paths, improving resilience and scalability. Developer productivity\nAWS Amplify simplifies frontend CI/CD and hosting. Container workflows with Docker + ECR and ECS Fargate enable reproducible deployments for backend services. Security and best-practice highlights\nLeast-privilege IAM roles for services and cross-account access where needed. KMS-managed encryption for RDS and S3 objects. WAF and rate-limiting on API Gateway to mitigate application-level attacks. This architecture balances enterprise-grade security with cost-effective serverless patterns and provides a pragmatic path for incremental adoption of advanced features (observability, multi-region DR, Bedrock-powered AI).\n"},{"uri":"https://phatpham14.github.io/fcj-fa25/2-proposal/","title":"Proposal","tags":[],"description":"","content":"APEX-EV Electric Vehicle Service Platform 1. Executive Summary RenGen is a comprehensive management platform designed to digitize and optimize maintenance workflows at service centers. The system centrally manages the entire service lifecycle—from request intake and repair processing to customer care—helping to eliminate manual tasks and enhance efficiency. Leveraging the power of the AWS cloud, RenGen combines flexible container architecture on Amazon ECS Fargate with the intelligent processing capabilities of Generative AI through Amazon Bedrock. The solution integrates automated development processes (CI/CD) from GitLab, ensuring rapid deployment speeds, high security, and rigorous monitoring, delivering a superior experience for end-users.\n2. Problem Statement What’s the Problem? Current operational processes rely heavily on manual methods, leading to inefficiencies, fragmented data, and a lack of intelligent support tools for automated customer interaction.\nThe Solution The platform employs a modern architecture, starting at the Edge layer with Amazon Route 53 for user routing. The interface (Frontend) is hosted on AWS Amplify Hosting, ensuring fast and stable access. Amazon API Gateway acts as the central hub, intelligently routing requests.\nTo ensure security, critical components such as ECS Fargate and the Amazon RDS database are placed in a Private Subnet, completely isolated from the public internet. Image data is stored on Amazon S3 and accessed securely via S3 Endpoints. Additionally, the software development process is fully automated: source code from GitLab is packaged and pushed to Amazon ECR for deployment to ECS.\nBenefits and Return on Investment Adopting this architecture delivers a significant competitive advantage by integrating Artificial Intelligence (GenAI) via Amazon Bedrock, which helps automate customer care and data analysis. The system ensures high availability and data security thanks to the VPC network separation design (Public/Private subnets).\nThe CI/CD process integrated with GitLab and ECR helps minimize downtime when updating new features, while Amazon CloudWatch provides comprehensive monitoring to detect incidents instantly. The cost model is optimized thanks to the use of Fargate (Serverless container) and Lambda (Pay-per-use), ensuring businesses only pay for the actual resources used. This investment not only resolves current operational challenges but also creates a solid technological foundation for long-term growth, with the expected Return on Investment (ROI) period significantly shortened.\n3. Solution Architecture The RenGen management platform utilizes a modern architecture deployed on AWS (Region ap-southeast-2), initiated by user access via Amazon Route 53 at the Edge layer. The User Interface (Frontend) is hosted on AWS Amplify Hosting, which establishes a direct connection to Amazon API Gateway as the central entry point.\nFrom the API Gateway, the data flow is strategically divided into three distinct paths:\nAI Tasks: Requests are routed to AWS Lambda to interact with Amazon Bedrock for generative AI capabilities. Notification Tasks: Asynchronous requests trigger AWS Lambda to handle email communications via Amazon SES. Core Business Logic: Traffic is directed through an Application Load Balancer (ALB) located in the Public Subnet, then forwarded to Amazon ECS Fargate instances secured within a Private Subnet. Data \u0026amp; Security:\nRelational data is persistently stored in Amazon RDS within the Private Subnet. To optimize security and performance, the architecture utilizes VPC Endpoints to keep traffic strictly within the AWS internal network:\nStatic assets and images stored in Amazon S3 are accessed securely via S3 Endpoints. Container images are pulled directly from Amazon ECR via ECR Endpoints. By leveraging these endpoints, the system eliminates the need for a NAT Gateway, thereby reducing costs and minimizing public internet exposure.\nDevOps \u0026amp; Monitoring:\nGitLab is used for source code management and CI/CD, automatically pushing deployments to Amplify (Frontend) and container images to ECR (Backend). AWS Services Used Route 53: DNS service, responsible for routing the domain (Edge layer) to the application. AWS Amplify Hosting: Hosts the web interface (frontend) and can integrate with CDN/WAF. In the diagram, it receives traffic from Route 53. Amazon API Gateway: The main entry point (Gateway), receiving and routing all requests from the frontend/Amplify to processing services. AWS Lambda (Bedrock): Handles AI/Generative AI tasks (prediction/content generation) by communicating with Amazon Bedrock. AWS Lambda (SES): Handles asynchronous tasks, such as processing notifications to send emails via AWS SES. Amazon Bedrock: General AI service (Gen AI), providing foundation models to execute intelligent business operations. AWS SES: Email sending service, performs the sending of notifications, quotes, or processing results from Lambda. VPC: Virtual network environment containing and protecting AWS resources (like ALB, ECS Fargate, RDS). ALB (Application Load Balancer): Load balancer, distributing traffic from API Gateway to application containers running on ECS Fargate. Amazon ECS Fargate: Runs the backend application as containers without server management, handling core business logic. Amazon RDS: Provides a relational database, placed in a Private Subnet to store structured data. Amazon S3: Stores multimedia files like photos or other large data. ECR: Repository for application container images (Docker), used by ECS Fargate for deployment. AWS CloudWatch: Monitoring service, collecting logs and metrics from the entire system to track performance and detect issues. Component Design Request Handling: Amazon Route 53 routes user domain requests to AWS Amplify Hosting, where the frontend interface is hosted. From there, API requests are forwarded to Amazon API Gateway, which acts as the central entry point to receive and route all incoming traffic.\nBusiness Logic Processing:\nCore Logic: All primary business operations are handled by containerized applications running on Amazon ECS Fargate, deployed within a Private Subnet to ensure maximum security. AI \u0026amp; Asynchronous Tasks: Generative AI tasks are processed by AWS Lambda interacting with Amazon Bedrock. Auxiliary tasks, such as email notifications, are handled by separate AWS Lambda functions triggering Amazon SES. Network Infrastructure:\nPublic Subnet: Hosts the Application Load Balancer (ALB) to receive and distribute external traffic. Private Subnet: Dedicated to sensitive resources including ECS Fargate and Amazon RDS, ensuring they are isolated from direct public internet access. VPC Endpoints: The system explicitly utilizes S3 Endpoints and ECR Endpoints. This design allows ECS Fargate to pull container images and access file storage securely within the AWS internal network, without traversing the public internet. Data Storage:\nAmazon RDS: Stores sensitive, structured relational data. Amazon S3: Stores multimedia files and large datasets. Deployment and Monitoring: The deployment pipeline is managed via GitLab, which triggers updates to AWS Amplify (Frontend) and pushes Docker images to Amazon ECR (Backend). Amazon CloudWatch provides comprehensive monitoring of performance logs and metrics across all services, from ECS and Lambda to RDS.\n4. Technical Implementation Implementation Phases The development project for the RenGen Smart Electric Vehicle Maintenance Platform — including the integration of an AI virtual assistant and a service management system — undergoes 4 phases:\nResearch and Architectural Design: Research suitable technologies (React.js, Spring Boot, AWS Bedrock) and design a system architecture combining Containers (ECS) and Serverless (Lambda) on AWS (1 month prior to commencement). Cost Estimation and Feasibility Check: Use the AWS Pricing Calculator to estimate operating costs for core services such as ECS Fargate, RDS, and token costs for Amazon Bedrock, and propose the most feasible solution. Architecture Adjustment for Cost/Solution Optimization: Refine the architecture, select appropriate configurations for ECS Fargate and RDS, and optimize Lambda runtime (timeouts) to balance AI processing performance and cost. Development, Testing, and Deployment: Program the React.js application (Frontend) and Spring Boot (Backend), integrate the Bedrock Agent, deploy CI/CD pipelines via GitLab, package Docker images to ECR, and launch operations on ECS. Technical Requirements Technical Requirements User Interface (Frontend): Practical knowledge of React.js to build scheduling interfaces and chat with the AI virtual assistant. Use AWS Amplify to automate the deployment process (Hosting), connect with Amazon API Gateway to send secure processing requests, ensuring a smooth user experience on all devices. Core System (Backend \u0026amp; Infrastructure): In-depth knowledge of Java/Spring Boot to develop maintenance business logic. The application is packaged using Docker, with images stored on AWS ECR and running on Amazon ECS Fargate. Requires understanding of Amazon RDS for relational databases (storing vehicle profiles, maintenance history). Specifically, requires AWS Lambda (Python) programming skills to connect with Amazon Bedrock (AI/Chatbot processing) and AWS SES (sending asynchronous email notifications). Manage detailed user authentication and authorization (customers/technicians) via Amazon Cognito. 5. Timeline \u0026amp; Milestones Project Timeline\nPhase 1 (Week 1-2): Design and Foundation:: Analyze \u0026amp; Design detailed AWS architecture (VPC, Subnets, Security Groups). Design Database (RDS Schema) and define APIs (Swagger/OpenAPI). Configure infrastructure environment: Setup VPC (Public/Private Subnets), IAM Roles, and Amazon Cognito (User Pools). Setup CI/CD: Configure Pipeline on GitLab to automatically build Docker Images, push to Amazon ECR, and deploy Frontend to AWS Amplify. Phase 2 (Week 3-4): Core Service Flow Development:: Develop Customer flow (Frontend/Backend): Registration/Login, Vehicle Profile Management, Appointment Scheduling (stored in RDS). Develop Service Advisor flow: Vehicle Reception, Create Quotations and Repair Orders. Phát triển luồng Kỹ thuật viên: Xem danh sách việc cần làm (Task list), Cập nhật tiến độ bảo dưỡng và tải ảnh/video lên Amazon S3. Phase 3 (Week 5-6): Administration \u0026amp; Advanced Feature Development:: Build Administration Module: Report Dashboard, Spare Parts Management (Inventory), and Personnel Management.Build Administration Module: Report Dashboard, Spare Parts Management (Inventory), and Personnel Management. Write AWS Lambda to connect Amazon Bedrock Agent (AI Chatbot for customer support) and expose via API Gateway. Write AWS Lambda to trigger AWS SES for sending automatic notification emails/quotations to customers. Configure NAT Gateway so resources in Private Subnet (Lambda, ECS) can securely connect to the Internet/AWS Services. Phase 4 (Week 7-8): Testing, Optimization, and Operation:: Internal User Acceptance Testing (UAT) to ensure the flow from Web -\u0026gt; API Gateway -\u0026gt; Lambda/ECS -\u0026gt; DB operates smoothly. Optimize security: Configure AWS WAF (block SQL Injection, XSS) and review IAM access rights. Operational monitoring: Setup Dashboard on Amazon CloudWatch to track logs and metrics of ECS Fargate and Lambda. Official deployment. 6. Budget Estimation Infrastructure Costs\nAmazon ECS Fargate: ~11.00 USD/month. Application Load Balancer (ALB): ~16.43 USD/month. Amazon Bedrock (AI): ~5.00 USD/month (Calculated by Token count). AWS Lambda: 0.00 USD/month (Free Tier). Amazon RDS \u0026amp; ElastiCache: 0.00 USD/month (Free Tier). S3 Standard: ~0.15 USD/month. AWS Amplify \u0026amp; API Gateway: ~0.50 USD/month. Amazon CloudWatch: 0.00 USD/month (Free Tier). Amazon SES: 0.00 USD/month (Free Tier). Total: ~32.63/month.\n7. Risk Assessment Risk Matrix System downtime: High impact, low probability. Security breach/Data loss: Very high impact, low probability. Operational cost overrun: Medium impact, medium probability. Mitigation Strategies System: Deploy infrastructure across Multi-AZ for RDS and ECS Fargate. Use Application Load Balancer for automatic load distribution and recovery. Security: Use AWS WAF to filter malicious requests. Strict authorization with Amazon Cognito and apply least privilege principle. Backend placed in a separate network (Private Subnet). Cost: Use AWS Budgets to set alerts when costs exceed thresholds. Regularly monitor and optimize resources (right-sizing) to avoid waste. Incorrect AI response: Medium impact, medium probability. Contingency Plans System: Deploy ECS Fargate and RDS infrastructure across Multi-AZ to ensure high availability. Use Application Load Balancer for automatic load coordination and Auto-scaling to expand Tasks when traffic spikes. AI Quality: Limit Bedrock Agent response scope via strict Prompt Engineering (System Prompts) and only allow information retrieval from moderated Knowledge Bases. Enable Automated Backups for RDS and Point-in-time Recovery to restore data to any point in time. 8. Expected Outcomes Technical Improvements: Technical Improvements: Successfully build a modern Hybrid Architecture combining Microservices (ECS Fargate) and Serverless (Lambda, Bedrock), ensuring flexible scalability without managing physical servers.\nLong-term Value Enhance customer experience: AI virtual assistant operating 24/7 helps reduce waiting time, increasing appointment conversion rates and car owner satisfaction.\nData assets: Maintenance history and interaction behavior data are centrally stored on RDS/S3, creating a premise for deploying AI Predictive Maintenance models for electric vehicle batteries and motors in the future.\n"},{"uri":"https://phatpham14.github.io/fcj-fa25/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Deep dive into IAM (Identity and Access Management) and AWS CLI configuration. Begin the \u0026ldquo;Understanding Research Methods\u0026rdquo; course on Coursera. Continue practicing Core Services: S3 and Storage. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - AWS: Learn about IAM details (Users, Groups, Roles, Policies). - Practice creating IAM users and assigning permissions (Least Privilege principle). 09/15/2025 09/15/2025 https://cloudjourney.awsstudygroup.com/ 2 - AWS: Install and configure AWS CLI on local machine. - Practice basic CLI commands to interact with EC2 and S3. 09/16/2025 09/16/2025 https://cloudjourney.awsstudygroup.com/ 3 - Coursera: Start Module 1: \u0026ldquo;What Is Research and What Makes a Good Research Question?\u0026rdquo; - Watch lectures and complete E-tivity 1. 09/17/2025 09/17/2025 https://www.coursera.org/learn/research-methods 4 - AWS: Learn about Amazon S3 (Classes, Security, Versioning). - Practice creating an S3 Bucket and hosting a Static Website. 09/18/2025 09/18/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Mastered security permission mechanisms with IAM. Proficient in using AWS CLI for resource management. Understood how to define a research question from the Coursera course. Successfully deployed a Static Website on S3. "},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.4-databaseandstorage/5.4.3-create-ecr/","title":"Create AWS ECR","tags":[],"description":"","content":" Open the Amazon Elastic Container Registry In create console, fill in repository name Choose Mutable in Image tag mutability Then click Create "},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.3-vpc/","title":"VPC","tags":[],"description":"","content":"Intro VPC (Virtual Private Cloud) is a logically isolated virtual private network space in AWS Cloud. It acts as your personal data center in the cloud, giving you complete control over the network environment.\nCore components: Route Table Subnets Internet Gateway Security Groups Create VPC Open the Amazon VPC console Choose Your VPCs, then click Create VPC In the create VPC console: Specify name of the Name tag: my-vpc-01 IPv4 CIDR : 10.0.0.0/16 Then click Create VPC Content Create Route Table \u0026amp; Internet Gateway Create Subnets Create Security Groups Create VPC Endpoints "},{"uri":"https://phatpham14.github.io/fcj-fa25/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section lists and introduces the technical blogs I have translated and studied during the internship. These articles cover various topics including Data Analytics, High Performance Computing (HPC), IoT, and Security.\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to build a healthcare data lake using a microservices architecture. You will learn the importance of storing and analyzing diverse healthcare data (EMR, lab tests, IoT) and how microservices offer flexibility and scalability. The article guides you through setting up an ingestion pipeline for HL7v2 messages using AWS Lambda, Amazon SNS, and DynamoDB.\nBlog 2 - Characteristics of Financial Services HPC Workloads on the Cloud This article analyzes the technical attributes of compute-heavy workloads in the Financial Services Industry (FSI). It provides a decision tree to help organizations choose the right HPC platform on AWS—whether migrating existing commercial schedulers (like IBM Symphony) or building cloud-native solutions using AWS Batch or AWS Lambda—based on task duration and throughput requirements.\nBlog 3 - AWS IoT Greengrass Nucleus Lite Introduces Nucleus Lite, a lightweight, C-based edge runtime optimized for resource-constrained devices (under 512MB RAM). The blog compares it with the standard Java Nucleus runtime, highlighting benefits like reduced memory footprint, static memory allocation, and deep integration with systemd for embedded Linux systems.\nBlog 4 - Proactive Strategies for Cyber Resilience and Business Continuity Provides guidance for Public Sector organizations to operate resilient systems. It covers essential strategies such as implementing a multi-account environment, using Infrastructure as Code (IaC) for rapid recovery, maintaining separate recovery accounts with consistent Availability Zone IDs, and utilizing immutable backups to defend against ransomware.\nBlog 5 - The Frugal HPC Architect: Effective FinOps for HPC Explores best practices for managing costs in large-scale HPC workloads. Based on the \u0026ldquo;Frugal Architect\u0026rdquo; principles, it discusses strategies to reduce unit costs (using EC2 Spot, Savings Plans, and Graviton processors) and unit consumption (optimizing efficiency and observability) to balance flexibility, performance, and budget.\n"},{"uri":"https://phatpham14.github.io/fcj-fa25/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand Load Balancing (ELB) and Auto Scaling. Study Coursera Module 2: Literature Review. Begin brainstorming ideas for the Group Project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - AWS: Learn about Elastic Load Balancing (ALB, NLB) and Auto Scaling Groups. - Lab: Configure ALB to distribute traffic to 2 EC2 instances. 09/22/2025 09/22/2025 https://cloudjourney.awsstudygroup.com/ 2 - AWS: Learn about CloudWatch (Metrics, Alarms, Logs). - Create an Alarm to trigger when EC2 CPU exceeds 70%. 09/23/2025 09/23/2025 https://cloudjourney.awsstudygroup.com/ 3 - Coursera: Module 2: \u0026ldquo;What Is a Literature Review?\u0026rdquo; - Practice searching for and selecting reference materials. 09/24/2025 09/24/2025 https://www.coursera.org/learn/research-methods 4 - Project: Brainstorm project ideas (e.g., E-commerce site, Serverless App). - Analyze preliminary requirements. 09/25/2025 09/25/2025 Week 3 Achievements: Deployed a High Availability (HA) system using ELB and Auto Scaling. Learned how to monitor systems via CloudWatch. Understood the importance of a Literature Review. Defined the initial direction for the Group Project. "},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.4-databaseandstorage/","title":"Database And Storage","tags":[],"description":"","content":"Overview This project used three data storage services which are: AWS RDS(Relational Database Service) is a AWS service to config database and is located in Private Subnet AWS S3 (Simple Storage Service) is a AWS service to storage picture and video AWS ECR (Elastic Container Registry) is a AWS service to storage docker image Content Create RDS Create S3 Create ECR "},{"uri":"https://phatpham14.github.io/fcj-fa25/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Advanced Networking and Content Delivery. Study Coursera Module 3: Planning \u0026amp; Management. Finalize Architecture for the Group Project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - AWS: Learn about Amazon Route 53 (DNS) and CloudFront (CDN). - Lab: Configure Domain and CDN for the S3 Static Website. 09/29/2025 09/29/2025 https://cloudjourney.awsstudygroup.com/ 2 - Coursera: Module 3: \u0026ldquo;Planning and Management Skills for Research\u0026rdquo;. - Learn about time management and research design. 09/30/2025 09/30/2025 https://www.coursera.org/learn/research-methods 3 - Project: Draw the Architecture Diagram on AWS. - Determine services to use (VPC, Lambda, RDS/DynamoDB). 10/01/2025 10/01/2025 https://cloudjourney.awsstudygroup.com/ 4 - AWS: Research VPC Peering and Transit Gateway (Advanced Networking). 10/02/2025 10/02/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Understood DNS and how to optimize page load speeds with CloudFront. Completed the High-Level Architecture design for the Group Project. Acquired research planning skills from Coursera. "},{"uri":"https://phatpham14.github.io/fcj-fa25/4-eventparticipated/","title":"Participated Events","tags":[],"description":"","content":" In this section, I list the technology events and in-depth workshops I participated in during my internship. These were key milestones that helped me update the latest technology trends from AWS and reinforce practical skills for the capstone project.\nDuring the internship, I participated in 4 main events. Each event was a memorable experience providing new and useful knowledge, along with valuable networking opportunities.\nEvent 1: AWS Cloud Day Vietnam 2025 - HCMC Connect Edition Event Name: AWS Cloud Day Vietnam 2025 - HCMC Connect Edition\nDate \u0026amp; Time: 08:30, September 18, 2025\nLocation: AWS Vietnam Office, Floors 26 \u0026amp; 36, Bitexco Financial Tower, District 1, HCMC\nRole: Attendee\nDescription: Participated in the connect session dedicated to the Builders community in HCMC. Updated on new trends in Generative AI, Modernization, and joined technical Breakout Tracks on cloud migration strategies.\nEvent 2: AI/ML/GenAI on AWS Workshop Event Name: AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, District 1, HCMC\nRole: Attendee\nDescription: In-depth workshop on Machine Learning and Generative AI. Practiced building a Chatbot using Amazon Bedrock, learned MLOps processes with Amazon SageMaker, and explored Prompt Engineering techniques.\nEvent 3: DevOps on AWS Workshop Event Name: DevOps on AWS Workshop\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, District 1, HCMC\nRole: Attendee\nDescription: Explored DevOps culture, built complete automated CI/CD pipelines with AWS CodePipeline, managed infrastructure as code (IaC) using AWS CDK, and implemented system observability using AWS X-Ray.\nEvent 4: AWS Well-Architected Security Pillar Event Name: AWS Well-Architected Security Pillar Workshop\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, District 1, HCMC\nRole: Attendee\nDescription: Reinforced cloud security mindset based on the 5 security pillars. Practiced Incident Response scenarios, Identity management, and data protection according to AWS Well-Architected standards.\n"},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.5-computeandcontainer/","title":"Compute And Container","tags":[],"description":"","content":"IAM Role Create ecsTaskExecutionRole role to AWS ECS (Elastic Container Service) pull docker image and write logs from ECR repository Open the Amazon IAM In left navbar, choose Roles, then click Create role In create console, choose AWS service and select Use case is EC2 then click Next In Add permissions step, search AmazonECSTaskExecutionRolePolicy and select this Policy, then click Next In Name, review, and create, fill in Role name is ecsTaskExecutionRole , then click Create role Create FargateTaskRole role to allow Fargate access resource In left navbar, choose Roles, then click Create role In create console, choose AWS service and select Use case is EC2 then click Next In Add permissions step, search AmazonS3FullAccess, AmazonSESFullAccess and AmazonSNSFullAccess, then click Next Review and fill in Role name is FargateTaskRole, then click Create role ECS Cluster Create ECS Cluster is place to Fargate running Open the Amazon ECS In left navbar, choose Clusters, then click Create Cluster In create console, fill in Cluster name In Infrastructure, select Fargate only Then click Create Task definitions Create Task definitions is a container design In left navbar of ECS console, choose Task definitions, then click **Create new task definitions In create console, fill in Task definition family Select AWS Fargate in Launch type of Infrastructure requirements Choose Task role is FargateTaskRole Choose Task execution role is ecsTaskExecutionRole In Container, fill in name and URL of ECR, then Add Environment variable Scroll down to the bottom and click Create ECS Service Create ECS Service to run Task definitions In left navbar, choose Clusters, then click Cluster created In tab Sevices, click Create In create console, choose Task definition family Choose Task definition revision Fill in Service name In Compute options choose Capacity provider strategy In Capacity provider, select FARGATE In Platform version, select LATEST In Deployment configuration, Scheduling strategy is Replica and Desired tasks is 1 Then click Create "},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Deploy ReGenZet Management System To AWS Overview ReGenZet is an enterprise-grade EV garage management platform. The objective of this workshop is to design and deploy a secure, cost-optimized, and highly automated cloud infrastructure on AWS to host ApexEV\u0026rsquo;s frontend, backend, storage, and serverless AI/ML functions.\nKey architectural principles:\nSecurity-first: least-privilege IAM, encrypted data at rest and in transit, network isolation and controlled service endpoints. Cost optimization: use managed services with pay-as-you-go models, right-sizing, and automated lifecycle policies for storage and compute. Automation \u0026amp; Observability: Infrastructure-as-Code, CI/CD pipelines, centralized logging, and automated monitoring/alarms. Core services used in this workshop:\nAWS ECS (Fargate) — run backend microservices without managing servers. AWS Amplify — host the frontend, provide CI/CD for web clients and manage hosting. Amazon RDS — managed relational database for transactional data. Amazon S3 — object storage for media, backups, and static assets. AWS Lambda — serverless functions for AI/ML processing pipelines, notifications and background tasks. This workshop contains hands-on modules covering the end-to-end stack and best practices for each layer.\nContent Workshop Overview Project Architecture VPC Of Project Database And Storage Compute And Container Create Load Balancing Create Amplify And API Gateway Intruct Deploy Code Frontend and Backend "},{"uri":"https://phatpham14.github.io/fcj-fa25/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Advanced Databases (NoSQL \u0026amp; Caching). Complete Coursera Course (Module 4). Start the Development (Dev) phase for the Project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - AWS: Learn about Amazon DynamoDB and ElastiCache. - Lab: Create a DynamoDB table and perform basic CRUD operations. 10/06/2025 10/06/2025 https://cloudjourney.awsstudygroup.com/ 2 - Coursera: Module 4: \u0026ldquo;Project Submission / Proposal\u0026rdquo;. - Complete the final assignment and Peer Review. Finished Coursera Course. 10/07/2025 10/07/2025 https://www.coursera.org/learn/research-methods 3 - Project: Setup development environment (IDE, Git repository). - Initialize Project (Init codebase). 10/08/2025 10/08/2025 4 - AWS: Practice integrating DynamoDB with Lambda functions. 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Completed the \u0026ldquo;Understanding Research Methods\u0026rdquo; Coursera certification. Distinguished between SQL (RDS) and NoSQL (DynamoDB). Group Project has officially entered the Coding phase. "},{"uri":"https://phatpham14.github.io/fcj-fa25/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services (AWS) - First Cloud Journey (FCJ) from September 10, 2025 to November 27, 2025, I had the valuable opportunity to gain both theoretical knowledge and practical hands-on experience on the AWS platform.\nI directly participated in deeply researching AWS services and building a Group Project with a hybrid architecture combining Serverless and Managed Services, thereby significantly improving skills in: System Architecture Design (Solution Architecture), DevOps implementation (CI/CD), Backend programming, Technical writing, and Teamwork.\nRegarding professional conduct, I maintained a high level of proactivity in my work and collaborated well with fellow FCJers. However, I am also clearly aware of my limitations regarding discipline and communication, allowing me to plan for future improvement.\nTo objectively reflect on the internship process, I would like to self-assess based on the criteria below:\nNo. Criteria Description Good Fair Average 1 Professional Knowledge \u0026amp; Skills Understanding of AWS, application in real projects, tool proficiency (Git, CLI, IDE), code quality, and architecture ☐ ✅ ☐ 2 Learning Ability Ability to acquire new technologies (Docker, Serverless, NoSQL) and best practices quickly ☐ ✅ ☐ 3 Proactivity Self-researching documentation, accepting difficult tasks, and proposing solutions without waiting for detailed instructions ✅ ☐ ☐ 4 Sense of Responsibility Commitment to completing work on time (Deadlines), ensuring project output quality ✅ ☐ ☐ 5 Discipline Adherence to timekeeping, regulations, workflows, and periodic reporting ☐ ✅ ☐ 6 Growth Mindset Willingness to receive feedback from Mentors/colleagues and making changes to improve oneself ☐ ✅ ☐ 7 Communication Presenting technical ideas, reporting progress, and discussing within the group ☐ ✅ ☐ 8 Team Collaboration Coordinating smoothly with members, supporting each other when encountering bugs, avoiding blame ✅ ☐ ☐ 9 Professional Conduct Respecting colleagues, Mentors, and the general culture of the FCJ organization ✅ ☐ ☐ 10 Problem Solving Mindset Identifying the root cause of errors, proposing optimal and creative solutions ☐ ✅ ☐ 11 Contribution to Project/Org Actual work efficiency, contribution to the success of the Final Showcase ✅ ☐ ☐ 12 Overall General assessment of the entire internship process and results achieved ✅ ☐ ☐ Personal Improvement Plan (Next Steps) Based on the assessment above, I recognize my strengths in technical expertise and proactivity, but I need to seriously address the following weaknesses to become a more professional engineer:\nEnhance Discipline: I realize I need to be stricter with myself regarding adherence to company regulations. Improve Problem-Solving Mindset: Although I can resolve tasks, I want to cultivate a multidimensional perspective. Instead of just finding a solution to \u0026ldquo;make the code run,\u0026rdquo; I will focus on deeply analyzing the performance, cost, and security of that solution. Learn Effective Communication: I will focus on improving my presentation skills, specifically articulating complex technical issues concisely and clearly. Simultaneously, I will practice calmness and tact when handling conflicting situations or defending personal viewpoints. "},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.7-amplifyandapigateway/","title":"Create Amplify And API Gateway","tags":[],"description":"","content":"AWS Amplify Create Amplify to deploy Frontend Open the Amazon Amplify Click Create new app In create console, choose Gitlab, then click Next In step Add repository and branch, login with Gitlab, then choose Git Repository and branch need to deploy Then click Next Setting format with your type code in your Frontend, then click Next Review and click Save and deploy After this step wait about 3-5 minutes to deploy and you can access your app from internet API Gateway API Gateway is service to transfer HTTP and HTTPS between Amplify (Frontend) and Fargate (Backend) Open the Amazon API Gateway Click Create API Choose REST API and click Build In create console, choose API details is New API Fill in API name API endpoint type is Regional Security policy is SecurityPolicy_TLS13_1_2_2021_06 Then click Create API In left navbar, choose APIs and select API Gateway created In console API Gateway click Create Resource then fill in Resource name and click Create resource In console resource API Gateway, click Create method In Method details choose ANY, and Integration type is HTTP Proxy HTTP method select ANY, and Endpoint URL is Endpoints of ALB Then scroll down to the bottom and click Create method In resource proxy, create method In create console, choose Integration type is Mock and Method type is OPTIONS Then click Create method Now we finish setup API Gateway for project "},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.6-createalb/","title":"Create AWS Load Balancers","tags":[],"description":"","content":" Open the Amazon EC2 In left navbar, choose Load Balancers, then click Create load balancer Choose Application Load Balancer In create console, fill in Load balancer name Scheme is Internet-facing, then select VPC In Availability Zones and subnets choose two AZ and select two subnet in public\\ Security groups select alb-sg Then scroll down to the bottom and click Create load balancer "},{"uri":"https://phatpham14.github.io/fcj-fa25/5-workshop/5.8-instruct-deploy-be-fe/","title":"Instruct Deploy BackEnd And Frontend","tags":[],"description":"","content":"Deploy Frontend Instructure Deploy Frontend to Amplify Open terminal in your code from your compute Commit and Push to branch of Gitlab which define in Amplify Amplify auto CICD and deploy your Frontend Wait 3-5 minutes Deploy Backend Instructure Deploy Backend to Fargate Open the Amazon ECR In left navbar, choose Repository and select ECR repository created Open terminal in your code from your compute Write syntax \u0026ldquo;AWS configure\u0026rdquo; to setup access key and secret key of your AWS account After setup AWS account for CLI, comeback console of ECR Repository Click View push command, you will see syntax to push docker image to ECR repository Then copy and paste it to terminal of your Backend project Fisnish this step, AWS ECS Fargate will auto pull and run image have tag latest Finally,Congratulations for finish this workshop with deploy success project to AWS. "},{"uri":"https://phatpham14.github.io/fcj-fa25/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Focus on Group Project development: Backend API. Learn about Containers on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - AWS: Learn about Docker and Amazon ECS/EKS or Lightsail Containers. - Create a Dockerfile for a sample app. 10/13/2025 10/13/2025 https://cloudjourney.awsstudygroup.com/ 2 - Project: Code main Backend functions (Authentication, User Management). - Design API Specs. 10/14/2025 10/14/2025 3 - Project: Connect Backend with Database (RDS/DynamoDB). 10/15/2025 10/15/2025 4 - AWS: Lab: Deploy a simple container app to ECS Fargate. 10/16/2025 10/16/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Understood Containerization concepts and ECS. Completed core modules of the Backend project. "},{"uri":"https://phatpham14.github.io/fcj-fa25/7-feedback/","title":"Sharing &amp; Feedback","tags":[],"description":"","content":" Here are my personal thoughts on my journey participating in the First Cloud Journey program. These opinions aim to help the FCJ team improve even further in future seasons.\nGeneral Assessment 1. Working Environment The environment at FCJ is extremely dynamic. I am very impressed by everyone\u0026rsquo;s openness; the working atmosphere is both professional and very comfortable, helping me reduce pressure when first approaching the vast amount of AWS knowledge. The workspace (both offline and online) created the best conditions for me to focus on the project.\n2. Mentor / Team Admin Support The Mentors are extremely dedicated. I really like how they guide us: not \u0026ldquo;spoon-feeding\u0026rdquo; answers but prompting issues so I can research AWS documentation myself and find the Solution. This helped me train my proactive thinking a lot. The Team Admin also enthusiastically supported processes, documentation, and credits so I wasn\u0026rsquo;t interrupted during lab practice.\n3. Alignment between Work and Major The internship work here is a perfect stepping stone for my major. At school, I learned about computer networks and basic programming, but at FCJ, I applied them in reality with Cloud Computing. Building a Hybrid Architecture combined with Serverless is a valuable practical experience that school hasn\u0026rsquo;t fully provided.\n4. Learning \u0026amp; Skill Development Opportunities This is the criterion I rate highest. In 12 weeks, I not only learned AWS services (ECS, Lambda, DynamoDB\u0026hellip;) but also professional software development processes (CI/CD, DevOps). Besides hard skills, I also learned how to write concise technical documentation and presentation skills to defend my solution.\n5. Culture \u0026amp; Team Spirit FCJ\u0026rsquo;s \u0026ldquo;Build - Learn - Share\u0026rdquo; culture truly spreads. During the Group Project, despite heated debates about architecture or encountering difficult bugs right before demo day, the whole group still \u0026ldquo;carried the team\u0026rdquo; together and supported each other fully. I felt I truly belonged to this collective, not just an intern passing through.\n6. Intern Policies / Benefits Being granted an AWS Sandbox account to practice freely without worrying about personal costs is a huge benefit for students like me. Additionally, internal workshops and career orientation sharing from seniors were very practical.\nOther Questions 1. What were you most satisfied with during the internship? It is the sense of \u0026ldquo;ownership\u0026rdquo;. I was trusted to build important modules in a real project, allowed to make mistakes and fix them. Seeing the system I built run smoothly during the Final Showcase was what satisfied me most.\n2. What do you think the company needs to improve for future interns? Honestly, looking back at the whole journey accompanying my peers as well as the seniors, I cannot find any points that need improvement because everything is currently too okay. From detailed instruction documents to clear regulations, everything has helped me a lot. Besides that, there is only a tiny, insignificant point about the group chat (specifically in the \u0026ldquo;Proposals \u0026amp; Wishes\u0026rdquo; section).\n3. Would you recommend this internship to your friends? Why? Definitely yes. I will advise friends passionate about Cloud to join. Because this is not a \u0026ldquo;fetching coffee\u0026rdquo; internship, but a place for real \u0026ldquo;combat\u0026rdquo;. The pressure might be high, but the knowledge and maturity gained are completely worth it.\nProposals \u0026amp; Wishes 1. Do you have any suggestions to improve the internship experience? I suggest having additional WhatsApp groups. One group dedicated to important announcements from mentors or team admins, and another group specifically for sharing experiences, answering questions, etc. This will avoid the case of messages getting buried and us missing updates from the seniors.\n2. Do you want to continue this program in the future? I very much look forward to the opportunity to continue participating in the program.\n"},{"uri":"https://phatpham14.github.io/fcj-fa25/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Understand DevOps \u0026amp; CI/CD workflows. Set up an automated Pipeline for the Group Project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - AWS: Learn about Developer Tools: CodeCommit, CodeBuild, CodeDeploy, CodePipeline. 10/20/2025 10/20/2025 https://cloudjourney.awsstudygroup.com/ 2 - Project: Write basic Unit Tests for Backend. 10/21/2025 10/21/2025 3 - Project: Configure CodePipeline to automatically build and deploy code to the Test environment upon new commits. 10/22/2025 10/22/2025 https://cloudjourney.awsstudygroup.com/ 4 - AWS: Lab: Practice CI/CD for a Static Web App. 10/23/2025 10/23/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Mastered CI/CD processes on AWS. Project system now supports automated deployment, reducing manual operations. "},{"uri":"https://phatpham14.github.io/fcj-fa25/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Develop Frontend and System Integration. Learn Advanced Serverless (API Gateway, EventBridge). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - AWS: Deep dive into API Gateway (Throttling, Caching, Keys) and EventBridge. 10/27/2025 10/27/2025 https://cloudjourney.awsstudygroup.com/ 2 - Project: Finalize Frontend interface (UI/UX). - Integrate Frontend with Backend APIs. 10/28/2025 10/28/2025 3 - Project: Fix integration bugs and issues. 10/29/2025 10/29/2025 4 - AWS: Lab: Build an Event-driven architecture with EventBridge. 10/30/2025 10/30/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Project main flow (Happy path) works from Frontend to Database. Understood Event-driven architecture concepts. "},{"uri":"https://phatpham14.github.io/fcj-fa25/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Enhance System Security. Code Review and Project Refactoring. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - AWS: Learn about AWS WAF, Shield (DDoS protection), and KMS (Key Management). 11/03/2025 11/03/2025 https://cloudjourney.awsstudygroup.com/ 2 - Project: Audit Security Groups, close unnecessary ports. - Configure WAF to block malicious requests. 11/04/2025 11/04/2025 https://cloudjourney.awsstudygroup.com/ 3 - Project: Cross-member Code Review. Refactor code for cleanliness and optimization. 11/05/2025 11/05/2025 4 - AWS: Learn about AWS Secrets Manager for managing DB passwords/API Keys. 11/06/2025 11/06/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: System secured with multiple layers (Network, Application, Data). Improved Project code quality. "},{"uri":"https://phatpham14.github.io/fcj-fa25/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Comprehensive Testing (Load Test) and Cost Optimization. Prepare for the packaging phase. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - AWS: Review AWS Cost Explorer \u0026amp; Budgets. Learn about Spot Instances/Savings Plans. 11/10/2025 11/10/2025 https://cloudjourney.awsstudygroup.com/ 2 - Project: Perform Load Testing (using JMeter or similar tools) to test capacity. 11/11/2025 11/11/2025 3 - Project: Adjust configurations (Resize EC2/RDS) based on test results to save costs. 11/12/2025 11/12/2025 4 - AWS: Learn about AWS Trusted Advisor for best practice recommendations. 11/13/2025 11/13/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Project ensured for performance and optimized operational costs. Understood how to manage costs on the Cloud. "},{"uri":"https://phatpham14.github.io/fcj-fa25/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Project Documentation. Finalize last-minute product details. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Project: Write Technical Documentation: Architecture, API Docs, Setup Guide. 11/17/2025 11/17/2025 2 - Project: Write Project Final Report (Applying reporting skills from Coursera). 11/18/2025 11/18/2025 3 - Project: Fix remaining minor bugs (UI glitches). Polish the product. 11/19/2025 11/19/2025 4 - Team: Internal retrospective meeting to discuss lessons learned. 11/20/2025 11/20/2025 Week 11 Achievements: Completed full project documentation set. Product is ready for the demo session. "},{"uri":"https://phatpham14.github.io/fcj-fa25/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Finalize Presentation Slides and Demo Script. Conduct Rehearsals and final review. Prepare mentally and technically for the Final Showcase. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Documentation: Finalize Presentation Slides (Introduction, Architecture, Demo, Key Takeaways). - Technical: Verify the Demo environment (ensure server/app stability). 11/24/2025 11/24/2025 Tue - Rehearsal: Dry run the demo script (solo or with the team) to check timing. - Bug Fix: Address minor issues (if any) found during the dry run. 11/25/2025 11/25/2025 Wed - Final Check: Check network, microphone, camera (if online) or equipment (if offline). - Backup: Prepare a backup plan (pre-recorded demo video) in case of live demo failure. 11/26/2025 11/26/2025 Thu - Portfolio: Update project info in CV/Portfolio (marked as \u0026ldquo;In Progress\u0026rdquo; or \u0026ldquo;Ready to showcase\u0026rdquo;). - Rest: Rest and maintain a positive mindset before the official day. 11/27/2025 11/27/2025 Week 12 Achievements: Product: Project system is stable, Demo script runs smoothly (Happy path). Documentation: Finalized Presentation Slides and technical documentation. Readiness: Mastered the presentation flow, prepared for Q\u0026amp;A from Mentors/Judges. Status: 100% Ready for the upcoming Final Showcase. "},{"uri":"https://phatpham14.github.io/fcj-fa25/tags/aws-public-sector/","title":"AWS Public Sector","tags":[],"description":"","content":""},{"uri":"https://phatpham14.github.io/fcj-fa25/tags/best-practices/","title":"Best Practices","tags":[],"description":"","content":""},{"uri":"https://phatpham14.github.io/fcj-fa25/tags/disaster-response/","title":"Disaster Response","tags":[],"description":"","content":""},{"uri":"https://phatpham14.github.io/fcj-fa25/tags/security/","title":"Security","tags":[],"description":"","content":""},{"uri":"https://phatpham14.github.io/fcj-fa25/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"https://phatpham14.github.io/fcj-fa25/tags/aws-iot/","title":"AWS IoT","tags":[],"description":"","content":""},{"uri":"https://phatpham14.github.io/fcj-fa25/tags/aws-iot-greengrass-v2/","title":"AWS IoT Greengrass V2","tags":[],"description":"","content":""},{"uri":"https://phatpham14.github.io/fcj-fa25/tags/edge-computing/","title":"Edge Computing","tags":[],"description":"","content":""},{"uri":"https://phatpham14.github.io/fcj-fa25/tags/fsi/","title":"FSI","tags":[],"description":"","content":""},{"uri":"https://phatpham14.github.io/fcj-fa25/tags/hpc/","title":"HPC","tags":[],"description":"","content":""},{"uri":"https://phatpham14.github.io/fcj-fa25/tags/iot-edge/","title":"IoT Edge","tags":[],"description":"","content":""},{"uri":"https://phatpham14.github.io/fcj-fa25/tags/finops/","title":"FinOps","tags":[],"description":"","content":""},{"uri":"https://phatpham14.github.io/fcj-fa25/categories/","title":"Categories","tags":[],"description":"","content":""}]